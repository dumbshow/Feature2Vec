{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building feat2vec\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from Feature2Vec import Feature2Vec \n",
    "from PLSR import PLSR\n",
    "from utils import * \n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"    \n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(seed = SEED)\n",
    "\n",
    "path = 'data/mcrae_feature_matrix.csv'\n",
    "#path = 'data/cslb_feature_matrix.csv'\n",
    "\n",
    "print('Building feature2vec')\n",
    "model = FeatureVec(path = path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get state needed to reproduce experiments from paper \n",
    "import pickle as pkl \n",
    "\n",
    "with open('state_zero.pkl', 'rb') as f:\n",
    "    st0 = pkl.load(f)\n",
    "np.random.set_state(st0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = np.random.permutation(len(model.concepts))\n",
    "train_concepts = list(np.asarray(model.concepts)[shuffle][:400])\n",
    "test_concepts = list(np.asarray(model.concepts)[shuffle][400:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training feat2vec\n",
      "Epoch: 19 Loss: 0.0026860087393787446\n"
     ]
    }
   ],
   "source": [
    "print('Training feature2vec')\n",
    "model.train(verbose = 1, epochs = 20, lr = 5e-3, negative_samples = 20, train_words = train_concepts)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example features learned for word: dog\n",
      "[['0.2758738349159628' 'has_a_wet_nose']\n",
      " ['0.2730467093512676' 'beh_-_barks']\n",
      " ['0.2724924595664283' 'has_4_legs']\n",
      " ['0.2700655491388194' 'is_domestic']\n",
      " ['0.2508572465465824' 'beh_-_chases']\n",
      " ['0.24673958423106437' 'has_fur']\n",
      " ['0.24578991111563422' 'beh_-_chases_cats']\n",
      " ['0.2441416081749012' 'has_a_tail']\n",
      " ['0.24304865630294026' 'a_pet']\n",
      " ['0.24184137316070192' 'a_carnivore']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test for word dog\n",
    "word = 'dog'\n",
    "print('Example features learned for word:', word)\n",
    "print(model.top_features(model.wvector(word), top = 10))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building partial least squared regression (50)\n",
      "Building partial least squared regression (120)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build baseline model (50 and 200)\n",
    "from PLSR import PLSR\n",
    "print('Building partial least squared regression (50)')\n",
    "plsr50 = PLSR(path = path)\n",
    "plsr50.set_vocabulary(train_concepts)\n",
    "plsr50.train(embedding_size = 50)\n",
    "\n",
    "print('Building partial least squared regression (120)')\n",
    "plsr120 = PLSR(path = path)\n",
    "plsr120.set_vocabulary(train_concepts)\n",
    "plsr120.train(embedding_size = 120)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_dict_plsr50 = {}\n",
    "for index, concept in enumerate(plsr50.test_words):\n",
    "    concept_dict_plsr50[concept] = plsr50.test_preds[index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_dict_plsr120 = {}\n",
    "for index, concept in enumerate(plsr120.test_words):\n",
    "    concept_dict_plsr120[concept] = plsr120.test_preds[index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLSR 50 neighbour scores\n",
      "Top 1 3.546099290780142\n",
      "Top 5 14.184397163120568\n",
      "Top 10 29.78723404255319\n",
      "Top 20 47.5177304964539\n"
     ]
    }
   ],
   "source": [
    "print('PLSR 50 neighbour scores')\n",
    "tops = [1, 5, 10, 20]\n",
    "for n in tops:\n",
    "    print('Top', n, neighbour_score(concept_dict_plsr50, plsr50, top = n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLSR 120 neighbour scores\n",
      "Top 1 3.546099290780142\n",
      "Top 5 25.53191489361702\n",
      "Top 10 43.262411347517734\n",
      "Top 20 53.90070921985816\n"
     ]
    }
   ],
   "source": [
    "print('PLSR 120 neighbour scores')\n",
    "tops = [1, 5, 10, 20]\n",
    "for n in tops:\n",
    "    print('Top', n, neighbour_score(concept_dict_plsr120, plsr120, top = n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_dict_f2v = {}\n",
    "for index, concept in enumerate(model.test_words):\n",
    "    concept_dict_f2v[concept] = construct_vector(concept, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 4.25531914893617\n",
      "Top 5 32.62411347517731\n",
      "Top 10 47.5177304964539\n",
      "Top 20 63.829787234042556\n"
     ]
    }
   ],
   "source": [
    "tops = [1, 5, 10, 20]\n",
    "for n in tops:\n",
    "    print('Top', n, neighbour_score(concept_dict_f2v, model, top = n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLSR 50 Scores\n",
      "Train: 49.523889981520554\n",
      "Test: 31.67454974804622\n"
     ]
    }
   ],
   "source": [
    "print('PLSR 50 Scores')\n",
    "print('Train:', np.mean(feature_score(plsr50, data_type = 'train', max_features = 0))*100)\n",
    "print('Test:', np.mean(feature_score(plsr50, data_type = 'test', max_features = 0))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLSR 120 Scores\n",
      "Train: 68.6591561040999\n",
      "Test: 32.96850865151701\n"
     ]
    }
   ],
   "source": [
    "print('PLSR 120 Scores')\n",
    "print('Train:', np.mean(feature_score(plsr120, data_type = 'train', max_features = 0))*100)\n",
    "print('Test:', np.mean(feature_score(plsr120, data_type = 'test', max_features = 0))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feat2Vec Scores\n",
      "Train: 89.47868798410427\n",
      "Test: 35.05511595558365\n"
     ]
    }
   ],
   "source": [
    "print('Feature2Vec Scores')\n",
    "print('Train:', np.mean(feature_score(model, data_type = 'train', max_features = 0))*100)\n",
    "print('Test:', np.mean(feature_score(model, data_type = 'test', max_features = 0))*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
